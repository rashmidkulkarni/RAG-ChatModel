{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10534302,"sourceType":"datasetVersion","datasetId":6517972},{"sourceId":10534321,"sourceType":"datasetVersion","datasetId":6517983}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:53:39.022041Z","iopub.execute_input":"2025-01-21T12:53:39.022359Z","iopub.status.idle":"2025-01-21T12:53:39.338706Z","shell.execute_reply.started":"2025-01-21T12:53:39.022331Z","shell.execute_reply":"2025-01-21T12:53:39.338027Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/requirements-txt/requirements.txt\n/kaggle/input/paper-pdf/paper.pdf\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install torch torchvision torchaudio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:53:39.339641Z","iopub.execute_input":"2025-01-21T12:53:39.340032Z","iopub.status.idle":"2025-01-21T12:53:43.238244Z","shell.execute_reply.started":"2025-01-21T12:53:39.340008Z","shell.execute_reply":"2025-01-21T12:53:43.237391Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install langchain_community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:53:43.239833Z","iopub.execute_input":"2025-01-21T12:53:43.240060Z","iopub.status.idle":"2025-01-21T12:53:51.768627Z","shell.execute_reply.started":"2025-01-21T12:53:43.240039Z","shell.execute_reply":"2025-01-21T12:53:51.767626Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain_community\n  Downloading langchain_community-0.3.15-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.10)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\nCollecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nCollecting langchain<0.4.0,>=0.3.15 (from langchain_community)\n  Downloading langchain-0.3.15-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-core<0.4.0,>=0.3.31 (from langchain_community)\n  Downloading langchain_core-0.3.31-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\nRequirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.24.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.15->langchain_community) (0.3.3)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.15->langchain_community) (2.10.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain_community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain_community) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain_community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain_community) (2.4.1)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.12.14)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.31->langchain_community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.15->langchain_community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.15->langchain_community) (2.27.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain_community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain_community) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.22.4->langchain_community) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.22.4->langchain_community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.22.4->langchain_community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.2.2)\nDownloading langchain_community-0.3.15-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain-0.3.15-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.31-py3-none-any.whl (412 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nInstalling collected packages: python-dotenv, httpx-sse, pydantic-settings, langchain-core, langchain, langchain_community\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.25\n    Uninstalling langchain-core-0.3.25:\n      Successfully uninstalled langchain-core-0.3.25\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.12\n    Uninstalling langchain-0.3.12:\n      Successfully uninstalled langchain-0.3.12\nSuccessfully installed httpx-sse-0.4.0 langchain-0.3.15 langchain-core-0.3.31 langchain_community-0.3.15 pydantic-settings-2.7.1 python-dotenv-1.0.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# from langchain.llms import HuggingFacePipeline\n# from transformers import pipeline\n\n# generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-2.7B\")\n# llm = HuggingFacePipeline(pipeline=generator)\n\n# response = llm(\"What is the purpose of life?\")\n# print(\"RESPONSE-------\")\n# print(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:53:51.770052Z","iopub.execute_input":"2025-01-21T12:53:51.770291Z","iopub.status.idle":"2025-01-21T12:53:51.774037Z","shell.execute_reply.started":"2025-01-21T12:53:51.770268Z","shell.execute_reply":"2025-01-21T12:53:51.773040Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# import os\n# import warnings\n# from typing import List\n# from dotenv import load_dotenv\n# from langchain_community import document_loaders\n# from langchain_community.document_loaders import PyPDFLoader\n# from langchain_huggingface import HuggingFaceEmbeddings\n# from langchain_community.vectorstores import Chroma\n# from langchain_core.documents import Document\n# from langchain.llms import HuggingFacePipeline\n# from transformers import pipeline\n# from langchain.text_splitter import RecursiveCharacterTextSplitter\n# from langchain.chains.retrieval_qa.base import RetrievalQA\n# from torch import autocast\n\n# # Load environment variables\n# load_dotenv()\n# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# warnings.filterwarnings('ignore')\n\n# def load_pdf(file_path: str) -> List[Document]:\n#     try:\n#         loader = PyPDFLoader(file_path)\n#         documents = loader.load()\n#         return documents\n#     except FileNotFoundError:\n#         print(f\"Error: File {file_path} not found.\")\n#         return []\n#     except Exception as e:\n#         print(f\"Error loading PDF: {e}\")\n#         return []\n\n# def split_documents(documents: List[Document], chunk_size: int = 500, chunk_overlap: int = 100) -> List[Document]:\n#     \"\"\"\n#     Split documents into smaller chunks.\n    \n#     Args:\n#         documents (List[Document]): Input documents\n#         chunk_size (int): Size of each text chunk\n#         chunk_overlap (int): Number of characters to overlap between chunks\n    \n#     Returns:\n#         List[Document]: Split documents\n#     \"\"\"\n#     text_splitter = RecursiveCharacterTextSplitter(\n#         chunk_size=chunk_size, \n#         chunk_overlap=chunk_overlap,\n#         separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n#     )\n#     return text_splitter.split_documents(documents)\n\n# def create_vector_store(documents: List[Document], embedding_model: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n#     \"\"\"\n#     Create a vector store from documents.\n    \n#     Args:\n#         documents (List[Document]): Input documents\n#         embedding_model (str): Hugging Face embedding model to use\n    \n#     Returns:\n#         Chroma: Vector store\n#     \"\"\"\n#     try:\n#         embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n#         return Chroma.from_documents(documents, embeddings)\n#     except Exception as e:\n#         print(f\"Error creating vector store: {e}\")\n#         return None\n\n# # def create_qa_chain(vector_store):\n# #     \"\"\"\n# #     Create a question-answering chain using EleutherAI GPT-Neo.\n    \n# #     Args:\n# #         vector_store: Vector store to use as retriever\n    \n# #     Returns:\n# #         RetrievalQA: Question-answering chain\n# #     \"\"\"\n# #     try:\n# #         # Load the EleutherAI GPT-Neo model\n# #         generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125M\", device=0)\n# #         llm = HuggingFacePipeline(pipeline=generator)\n        \n# #         retriever = vector_store.as_retriever(\n# #             search_type=\"similarity\", \n# #             search_kwargs={\"k\": 3}  # Retrieve top 3 most similar chunks\n# #         )\n# #         return RetrievalQA.from_chain_type(\n# #             llm=llm, \n# #             chain_type=\"stuff\", \n# #             retriever=retriever,\n# #             return_source_documents=True\n# #         )\n# #     except Exception as e:\n# #         print(f\"Error creating QA chain: {e}\")\n# #         return None\n# def create_qa_chain(vector_store):\n#     try:\n#         # Load the generator pipeline for your model\n#         generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125M\", device=0, batch_size=1)\n#         llm = HuggingFacePipeline(pipeline=generator)\n        \n#         # Increase document retrieval to provide more context\n#         retriever = vector_store.as_retriever(\n#             search_type=\"similarity\", \n#             search_kwargs={\"k\": 3}  # Retrieve top 3 documents\n#         )\n        \n#         # Ensure that the model answers based on the context\n#         return RetrievalQA.from_chain_type(\n#             llm=llm, \n#             chain_type=\"stuff\", \n#             retriever=retriever,\n#             return_source_documents=True,\n#             prompt=\"Using the information provided, please answer the question: {query}. If the answer isn't available, say 'I don't know.'\"\n#         )\n#     except Exception as e:\n#         print(f\"Error creating QA chain: {e}\")\n#         return None\n        \n# def main():\n#     # PDF file path\n#     pdf_path = \"/kaggle/input/paper-pdf/paper.pdf\"\n    \n#     # Load PDF\n#     documents = load_pdf(pdf_path)\n#     if not documents:\n#         return \"No Document was found.\"\n    \n#     # Split documents\n#     split_docs = split_documents(documents)\n    \n#     # Create vector store\n#     vector_store = create_vector_store(split_docs)\n#     if not vector_store:\n#         return \"No vector store\"\n    \n#     # Create QA chain\n#     qa_chain = create_qa_chain(vector_store)\n#     if not qa_chain:\n#         return\n    \n#     # Example query\n#     query = \"Who are the authors of the paper?\"\n    \n#     try:\n#         # Run query\n#         response = qa_chain.invoke({\"query\": query})\n        \n#         # Print response and source documents\n#         print(\"Answer:\", response['result'])\n#         print(\"\\nSource Documents:\")\n#         for doc in response['source_documents']:\n#             print(f\"- Page {doc.metadata.get('page', 'N/A')}: {doc.page_content[:200]}...\")\n    \n#     except Exception as e:\n#         print(f\"Error processing query: {e}\")\n\n# if __name__ == \"__main__\":\n#     main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:53:51.775027Z","iopub.execute_input":"2025-01-21T12:53:51.775306Z","iopub.status.idle":"2025-01-21T12:53:51.790116Z","shell.execute_reply.started":"2025-01-21T12:53:51.775272Z","shell.execute_reply":"2025-01-21T12:53:51.789411Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# import os\n# import warnings\n# from typing import List\n# from dotenv import load_dotenv\n# from langchain_community import document_loaders\n# from langchain_community.document_loaders import PyPDFLoader\n# from langchain_huggingface import HuggingFaceEmbeddings\n# from langchain_community.vectorstores import Chroma\n# from langchain_core.documents import Document\n# from langchain.llms import HuggingFacePipeline\n# from transformers import pipeline\n# from langchain.text_splitter import RecursiveCharacterTextSplitter\n# from langchain.chains.retrieval_qa.base import RetrievalQA\n# from langchain.prompts import PromptTemplate\n# from torch import autocast\n\n# # Load environment variables\n# load_dotenv()\n# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# warnings.filterwarnings('ignore')\n\n# def load_pdf(file_path: str) -> List[Document]:\n#     try:\n#         loader = PyPDFLoader(file_path)\n#         documents = loader.load()\n#         return documents\n#     except FileNotFoundError:\n#         print(f\"Error: File {file_path} not found.\")\n#         return []\n#     except Exception as e:\n#         print(f\"Error loading PDF: {e}\")\n#         return []\n\n# def split_documents(documents: List[Document], chunk_size: int = 500, chunk_overlap: int = 100) -> List[Document]:\n#     \"\"\"\n#     Split documents into smaller chunks.\n    \n#     Args:\n#         documents (List[Document]): Input documents\n#         chunk_size (int): Size of each text chunk\n#         chunk_overlap (int): Number of characters to overlap between chunks\n    \n#     Returns:\n#         List[Document]: Split documents\n#     \"\"\"\n#     text_splitter = RecursiveCharacterTextSplitter(\n#         chunk_size=chunk_size, \n#         chunk_overlap=chunk_overlap,\n#         separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n#     )\n#     return text_splitter.split_documents(documents)\n\n# def create_vector_store(documents: List[Document], embedding_model: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n#     \"\"\"\n#     Create a vector store from documents.\n    \n#     Args:\n#         documents (List[Document]): Input documents\n#         embedding_model (str): Hugging Face embedding model to use\n    \n#     Returns:\n#         Chroma: Vector store\n#     \"\"\"\n#     try:\n#         embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n#         return Chroma.from_documents(documents, embeddings)\n#     except Exception as e:\n#         print(f\"Error creating vector store: {e}\")\n#         return None\n\n# def create_qa_chain(vector_store):\n#     try:\n#         # Load the generator pipeline for your model\n#         generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125M\", device=0, batch_size=1)\n#         llm = HuggingFacePipeline(pipeline=generator)\n\n#         # Define the prompt template for the QA system\n#         prompt_template = \"\"\"\n#         Use the following pieces of context to answer the question at the end. \n#         If you don't know the answer, just say that you don't know, don't try to make up an answer.\n        \n#         {context}\n        \n#         Question: {query}\n#         Answer:\n#         \"\"\"\n        \n#         # Create a PromptTemplate object using the above template\n#         prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"query\"])\n\n#         # Increase document retrieval to provide more context\n#         retriever = vector_store.as_retriever(\n#             search_type=\"similarity\", \n#             search_kwargs={\"k\": 3}  # Retrieve top 3 documents\n#         )\n        \n#         # Use the prompt template to create the QA chain\n#         return RetrievalQA.from_chain_type(\n#             llm=llm, \n#             chain_type=\"stuff\", \n#             retriever=retriever,\n#             return_source_documents=True,\n#             prompt=prompt\n#         )\n#     except Exception as e:\n#         print(f\"Error creating QA chain: {e}\")\n#         return None\n        \n# def main():\n#     # PDF file path\n#     pdf_path = \"/kaggle/input/paper-pdf/paper.pdf\"\n    \n#     # Load PDF\n#     documents = load_pdf(pdf_path)\n#     if not documents:\n#         return \"No Document was found.\"\n    \n#     # Split documents\n#     split_docs = split_documents(documents)\n    \n#     # Create vector store\n#     vector_store = create_vector_store(split_docs)\n#     if not vector_store:\n#         return \"No vector store\"\n    \n#     # Create QA chain\n#     qa_chain = create_qa_chain(vector_store)\n#     if not qa_chain:\n#         return\n    \n#     # Example query\n#     query = \"Who are the authors of the paper?\"\n    \n#     try:\n#         # Fetch the top 3 relevant documents based on the query\n#         relevant_docs = qa_chain.retriever.get_relevant_documents(query)\n        \n#         # Format the context (documents) and use the query\n#         context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n        \n#         # Run query with the context and query variables\n#         response = qa_chain.invoke({\"query\": query, \"context\": context})\n        \n#         # Print response and source documents\n#         print(\"Answer:\", response['result'])\n#         print(\"\\nSource Documents:\")\n#         for doc in response['source_documents']:\n#             print(f\"- Page {doc.metadata.get('page', 'N/A')}: {doc.page_content[:200]}...\")\n    \n#     except Exception as e:\n#         print(f\"Error processing query: {e}\")\n\n# if __name__ == \"__main__\":\n#     main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:53:51.791069Z","iopub.execute_input":"2025-01-21T12:53:51.791342Z","iopub.status.idle":"2025-01-21T12:53:51.804268Z","shell.execute_reply.started":"2025-01-21T12:53:51.791313Z","shell.execute_reply":"2025-01-21T12:53:51.803689Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import os\nimport warnings\nfrom typing import List\nfrom dotenv import load_dotenv\nfrom langchain_community import document_loaders\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.documents import Document\nfrom langchain.llms import HuggingFacePipeline\nfrom transformers import pipeline\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.prompts import PromptTemplate\nfrom torch import autocast\nfrom langchain.chains import RetrievalQA\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import HuggingFacePipeline\nfrom transformers import pipeline\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain_community.vectorstores import Chroma\n\n# Load environment variables\nload_dotenv()\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nwarnings.filterwarnings('ignore')\n\ndef load_pdf(file_path: str) -> List[Document]:\n    try:\n        loader = PyPDFLoader(file_path)\n        documents = loader.load()\n        return documents\n    except FileNotFoundError:\n        print(f\"Error: File {file_path} not found.\")\n        return []\n    except Exception as e:\n        print(f\"Error loading PDF: {e}\")\n        return []\n\ndef split_documents(documents: List[Document], chunk_size: int = 500, chunk_overlap: int = 100) -> List[Document]:\n    \"\"\"\n    Split documents into smaller chunks.\n    \n    Args:\n        documents (List[Document]): Input documents\n        chunk_size (int): Size of each text chunk\n        chunk_overlap (int): Number of characters to overlap between chunks\n    \n    Returns:\n        List[Document]: Split documents\n    \"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size, \n        chunk_overlap=chunk_overlap,\n        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n    )\n    return text_splitter.split_documents(documents)\n\ndef create_vector_store(documents: List[Document], embedding_model: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n    \"\"\"\n    Create a vector store from documents.\n    \n    Args:\n        documents (List[Document]): Input documents\n        embedding_model (str): Hugging Face embedding model to use\n    \n    Returns:\n        Chroma: Vector store\n    \"\"\"\n    try:\n        embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n        return Chroma.from_documents(documents, embeddings)\n    except Exception as e:\n        print(f\"Error creating vector store: {e}\")\n        return None\n\ndef create_qa_chain(vector_store):\n    try:\n        # Load the generator pipeline for your model\n        generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125M\", device=0)\n        llm = HuggingFacePipeline(pipeline=generator)\n\n        # Create the retriever from the vector store\n        retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # Get top 3 docs\n\n        # Create a PromptTemplate for the question answering chain\n        prompt_template = PromptTemplate(\n            input_variables=[\"context\", \"query\"], \n            template=\"\"\"\n            Use the following pieces of context to answer the question at the end. \n            If you don't know the answer, just say that you don't know, don't try to make up an answer.\n            \n            {context}\n            \n            Question: {query}\n            Answer:\n            \"\"\"\n        )\n\n        # Load QA chain with 'stuff' combine_documents_chain\n        qa_chain = load_qa_chain(\n            llm=llm, \n            chain_type=\"stuff\",  # 'stuff' combine_documents_chain type\n            retriever=retriever,\n            combine_docs_chain=load_qa_chain(llm=llm, chain_type=\"stuff\"),  # Set the combine_documents_chain\n            prompt=prompt_template,\n            return_source_documents=True\n        )\n        \n        return qa_chain\n    except Exception as e:\n        print(f\"Error creating QA chain: {e}\")\n        return None\n\n        \ndef main():\n    # PDF file path\n    pdf_path = \"/kaggle/input/paper-pdf/paper.pdf\"\n    \n    # Load PDF\n    documents = load_pdf(pdf_path)\n    if not documents:\n        return \"No Document was found.\"\n    \n    # Split documents\n    split_docs = split_documents(documents)\n    \n    # Create vector store\n    vector_store = create_vector_store(split_docs)\n    if not vector_store:\n        return \"No vector store\"\n    \n    # Create QA chain\n    qa_chain = create_qa_chain(vector_store)\n    if not qa_chain:\n        return\n    \n    # Example query\n    query = \"Who are the authors of the paper?\"\n    \n    try:\n        # Fetch the top 3 relevant documents based on the query\n        relevant_docs = qa_chain.retriever.get_relevant_documents(query)\n        \n        # Format the context (documents) and use the query\n        context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n        \n        # Use the prompt template to generate the answer\n        formatted_prompt = qa_chain.llm.pipeline.tokenizer.encode(\n            prompt_template.format(context=context, query=query)\n        )\n        response = qa_chain.llm.pipeline(formatted_prompt)\n        \n        # Print response and source documents\n        print(\"Answer:\", response[0]['generated_text'])\n        print(\"\\nSource Documents:\")\n        for doc in relevant_docs:\n            print(f\"- Page {doc.metadata.get('page', 'N/A')}: {doc.page_content[:200]}...\")\n    \n    except Exception as e:\n        print(f\"Error processing query: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:53:51.805009Z","iopub.execute_input":"2025-01-21T12:53:51.805273Z","iopub.status.idle":"2025-01-21T12:53:52.985022Z","shell.execute_reply.started":"2025-01-21T12:53:51.805245Z","shell.execute_reply":"2025-01-21T12:53:52.983793Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-be061f8187ca>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdocument_loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_huggingface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocuments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_huggingface'"],"ename":"ModuleNotFoundError","evalue":"No module named 'langchain_huggingface'","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport warnings\nfrom typing import List\nfrom dotenv import load_dotenv\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.documents import Document\nfrom langchain.llms import HuggingFacePipeline\nfrom transformers import pipeline\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import RetrievalQA\nfrom langchain.chains.question_answering import load_qa_chain\n\n\nload_dotenv()\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nwarnings.filterwarnings('ignore')\n\ndef load_pdf(file_path: str) -> List[Document]:\n    try:\n        loader = PyPDFLoader(file_path)\n        documents = loader.load()\n        return documents\n    except FileNotFoundError:\n        print(f\"Error: File {file_path} not found.\")\n        return []\n    except Exception as e:\n        print(f\"Error loading PDF: {e}\")\n        return []\n\ndef split_documents(documents: List[Document], chunk_size: int = 500, chunk_overlap: int = 100) -> List[Document]:\n    \n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size, \n        chunk_overlap=chunk_overlap,\n        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n    )\n    return text_splitter.split_documents(documents)\n\ndef create_vector_store(documents: List[Document], embedding_model: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n    \n    try:\n        embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n        return Chroma.from_documents(documents, embeddings)\n    except Exception as e:\n        print(f\"Error creating vector store: {e}\")\n        return None\n\ndef create_qa_chain(vector_store):\n    try:\n        generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-neo-125M\", device=0,max_length = 512,temperature=0)\n        llm = HuggingFacePipeline(pipeline=generator)\n\n        retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  \n\n        prompt_template = PromptTemplate(\n            input_variables=[\"context\", \"query\"], \n            template=\"\"\"\n            Use the following pieces of context to answer the question at the end. \n            If you don't know the answer, just say that you don't know, don't try to make up an answer.\n            \n            {context}\n            \n            Question: {query}\n            Answer:\n            \"\"\"\n        )\n\n        qa_chain = RetrievalQA.from_chain_type(\n            llm=llm,\n            chain_type=\"stuff\",\n            retriever=retriever,\n            return_source_documents=True\n        )\n        \n        return qa_chain, prompt_template\n    except Exception as e:\n        print(f\"Error creating QA chain: {e}\")\n        return None, None\n\n        \ndef main():\n    pdf_path = \"/kaggle/input/paper-pdf/paper.pdf\"\n    \n    documents = load_pdf(pdf_path)\n    if not documents:\n        return \"No Document was found.\"\n    \n    split_docs = split_documents(documents)\n    \n    vector_store = create_vector_store(split_docs)\n    if not vector_store:\n        return \"No vector store\"\n    \n    qa_chain, prompt_template = create_qa_chain(vector_store)\n    if not qa_chain:\n        return\n    \n    query = \"What is the main conclusion of the research?\"\n    \n    try:\n        result = qa_chain({\"query\": query})\n        \n        print(\"Answer:\", result['result'])\n        print(\"\\nSource Documents:\")\n        for doc in result['source_documents']:\n            print(f\"- Page {doc.metadata.get('page', 'N/A')}: {doc.page_content[:200]}...\")\n    \n    except Exception as e:\n        print(f\"Error processing query: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n\n    if not documents:\n        return \"No Document was found.\"\n    \n    split_docs = split_documents(documents)\n    \n    vector_store = create_vector_store(split_docs)\n    if not vector_store:\n        return \"No vector store\"\n    \n    qa_chain, prompt_template = create_qa_chain(vector_store)\n    if not qa_chain:\n        return\n    \n    query = \"What is the main conclusion of the research?\"\n    \n    try:\n        result = qa_chain({\"query\": query})\n        \n        print(\"Answer:\", result['result'])\n        print(\"\\nSource Documents:\")\n        for doc in result['source_documents']:\n            print(f\"- Page {doc.metadata.get('page', 'N/A')}: {doc.page_content[:200]}...\")\n    \n    except Exception as e:\n        print(f\"Error processing query: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:55:44.937322Z","iopub.execute_input":"2025-01-21T12:55:44.937652Z","iopub.status.idle":"2025-01-21T12:56:19.070889Z","shell.execute_reply.started":"2025-01-21T12:55:44.937626Z","shell.execute_reply":"2025-01-21T12:56:19.070064Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11611128463445d0a07e6939263aac9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f1c97ad0e2404d90247c5331213428"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbae389cc5d54bb38b25dc7a350c8bb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b6794d6691a4b1b81f8309fe85b337a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d60a905e6be424eab914da0090d7a16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e40e48163e0c43faa1294ad60c333605"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de821bfc632f49a0aa72111371e5eeb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fd62098ca3c409b9d1199f59520e5da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03355dac27cb401592fa0e1d4119e9d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b01294dd5bc54722b13a7e3a9c96b96e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ec62cbeda594f9d9292b3c47c866661"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30ca1b6dd4a14865bffe5bbd82031c6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e38c73d03d54c96be5d618c366dc0fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"868d72fd94d34a21be6483a16041811f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c3b814375314a8fa25fbf71d5b00ccb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"230e30ef664844bab719c85abbff9fac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92c4979266524c29b015b67059f640ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dff50f7c4af14f1ba288d3a9d2b1b318"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9e1d8e264964aba92bf4ad60bbb46de"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nbook reviews. *e selected articles include both analytic\nstudies(primarilyqualitativeresearch)andempiricalstudies\n(primarily quantitative research).\n2.2. Performing the Review. Following Wu et al. [18], this\nstudy wasconductedintwo steps:identiﬁcationandcoding.\nIn the ﬁrst step, an article was selected to the potential pool\nwhen it qualiﬁed for either of two criteria: (a) the research\ninvolved a speciﬁc AI technique as an intervention in\n\neﬀects. *e review was conducted in three stages: planning,\nperforming, and reporting the systematic review.\n2.1. Planning the Review. As previous reviews about AIwere\nconducted in the physical sciences [16, 17], the study aimed\nto conduct a review in the ﬁeld of the social sciences.\n*e Web of Science database and the Social Science\nCitation Index (SSCI) journals were selected for the search\nfor desired articles published from 2010 to 2020. Articles\n\nsectorandexplorethepotentialresearchtrendsandchallengesofAIineducation.Atotalof100papersincluding63empiricalpapers(74\nstudies) and 37 analytic papers were selected from the education and educational research category of Social Sciences Citation Index\ndatabase from 2010 to 2020. *e content analysis showed that the research questions could be classiﬁed into development layer\n\nQuestion: What is the main conclusion of the research?\nHelpful Answer:\nThe main conclusion of the research is that AI is a\nmethod of learning.\n\n2.2. Performing the Review. Following Wu et al. [18], this\nstudy wasconductedintwo steps:identiﬁcationandcoding.\nIn the ﬁrst step, an article was selected to the potential pool\nwhen it qualiﬁed for either of two criteria: (a) the research\ninvolved a speciﬁc AI technique as an intervention in\n\neﬀects. *e review was conducted in three stages: planning,\nperforming, and reporting the\n\nSource Documents:\n- Page 1: book reviews. *e selected articles include both analytic\nstudies(primarilyqualitativeresearch)andempiricalstudies\n(primarily quantitative research).\n2.2. Performing the Review. Following Wu et al. [18...\n- Page 1: eﬀects. *e review was conducted in three stages: planning,\nperforming, and reporting the systematic review.\n2.1. Planning the Review. As previous reviews about AIwere\nconducted in the physical science...\n- Page 0: sectorandexplorethepotentialresearchtrendsandchallengesofAIineducation.Atotalof100papersincluding63empiricalpapers(74\nstudies) and 37 analytic papers were selected from the education and educational r...\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nimport warnings\nfrom typing import List\nfrom dotenv import load_dotenv\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.documents import Document\nfrom langchain.llms import HuggingFacePipeline\nfrom transformers import pipeline\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import RetrievalQA\nfrom datasets import Dataset, DatasetDict, Features, Value\n\n# Load environment variables\nload_dotenv()\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nwarnings.filterwarnings('ignore')\n\ndef load_pdf(file_path: str) -> List[Document]:\n    try:\n        loader = PyPDFLoader(file_path)\n        documents = loader.load()\n        return documents\n    except FileNotFoundError:\n        print(f\"Error: File {file_path} not found.\")\n        return []\n    except Exception as e:\n        print(f\"Error loading PDF: {e}\")\n        return []\n\ndef split_documents(documents: List[Document], chunk_size: int = 500, chunk_overlap: int = 100) -> List[Document]:\n    \"\"\"\n    Split documents into smaller chunks.\n    \n    Args:\n        documents (List[Document]): Input documents\n        chunk_size (int): Size of each text chunk\n        chunk_overlap (int): Number of characters to overlap between chunks\n    \n    Returns:\n        List[Document]: Split documents\n    \"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size, \n        chunk_overlap=chunk_overlap,\n        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n    )\n    return text_splitter.split_documents(documents)\n\ndef create_vector_store(documents: List[Document], embedding_model: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n    \"\"\"\n    Create a vector store from documents.\n    \n    Args:\n        documents (List[Document]): Input documents\n        embedding_model (str): Hugging Face embedding model to use\n    \n    Returns:\n        Chroma: Vector store\n    \"\"\"\n    try:\n        embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n        return Chroma.from_documents(documents, embeddings)\n    except Exception as e:\n        print(f\"Error creating vector store: {e}\")\n        return None\n\ndef create_qa_chain(vector_store):\n    try:\n        # Switch to a QA-specific model, e.g., deepset/roberta-base-squad2\n        qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", device=0)\n\n        # Create a HuggingFacePipeline\n        llm = HuggingFacePipeline(pipeline=qa_pipeline)\n\n        # Create the retriever from the vector store\n        retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})  # Get top 3 docs\n\n        # Initialize the RetrievalQA chain\n        qa_chain = RetrievalQA.from_chain_type(\n            llm=llm,  # Using the QA model for this chain\n            chain_type=\"stuff\",\n            retriever=retriever,\n            return_source_documents=True\n        )\n        \n        return qa_chain\n    except Exception as e:\n        print(f\"Error creating QA chain: {e}\")\n        return None\n\ndef main():\n    # PDF file path\n    pdf_path = \"/kaggle/input/paper-pdf/paper.pdf\"\n    \n    # Load PDF\n    documents = load_pdf(pdf_path)\n    if not documents:\n        return \"No Document was found.\"\n    \n    # Split documents\n    split_docs = split_documents(documents)\n    \n    # Create vector store\n    vector_store = create_vector_store(split_docs)\n    if not vector_store:\n        return \"No vector store\"\n    \n    # Create QA chain\n    qa_chain = create_qa_chain(vector_store)\n    if not qa_chain:\n        return\n    \n    # Example query\n    query = \"What is the main conclusion of the research?\"\n    \n    try:\n        # Retrieve relevant context from the documents\n        results = vector_store.similarity_search(query, k=3)\n        context = \" \".join([doc.page_content for doc in results])\n        \n        # Ensure input format is as expected by qa_chain\n        input_data = {\n            \"query\": query,\n            \"context\": context\n        }\n        \n        # Run the query using the RetrievalQA chain\n        result = qa_chain(input_data)  # Pass the dictionary directly\n\n        # Print result and source documents\n        print(\"Answer:\", result['result'])\n        print(\"\\nSource Documents:\")\n        for doc in result['source_documents']:\n            print(f\"- Page {doc.metadata.get('page', 'N/A')}: {doc.page_content[:200]}...\")\n    \n    except Exception as e:\n        print(f\"Error processing query: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:53:52.986464Z","iopub.status.idle":"2025-01-21T12:53:52.986799Z","shell.execute_reply":"2025-01-21T12:53:52.986674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langchain transformers PyPDF2 pdfplumber faiss-cpu langchain_huggingface\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:53:52.987505Z","iopub.status.idle":"2025-01-21T12:53:52.988079Z","shell.execute_reply":"2025-01-21T12:53:52.987941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -r /kaggle/input/requirements-txt/requirements.txt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:54:20.543486Z","iopub.execute_input":"2025-01-21T12:54:20.543840Z","iopub.status.idle":"2025-01-21T12:55:26.652832Z","shell.execute_reply.started":"2025-01-21T12:54:20.543806Z","shell.execute_reply":"2025-01-21T12:55:26.651895Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting aiohappyeyeballs==2.4.3 (from -r /kaggle/input/requirements-txt/requirements.txt (line 1))\n  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\nCollecting aiohttp==3.11.8 (from -r /kaggle/input/requirements-txt/requirements.txt (line 2))\n  Downloading aiohttp-3.11.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nCollecting aiosignal==1.3.1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 3))\n  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 4)) (0.7.0)\nCollecting anyio==4.6.2.post1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 5))\n  Downloading anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\nCollecting asgiref==3.8.1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 6))\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting attrs==24.2.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 7))\n  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\nCollecting backoff==2.2.1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 8))\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nCollecting bcrypt==4.2.1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 9))\n  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\nCollecting build==1.2.2.post1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 10))\n  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: cachetools==5.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 11)) (5.5.0)\nCollecting certifi==2024.8.30 (from -r /kaggle/input/requirements-txt/requirements.txt (line 12))\n  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: charset-normalizer==3.4.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 13)) (3.4.0)\nCollecting chroma-hnswlib==0.7.6 (from -r /kaggle/input/requirements-txt/requirements.txt (line 14))\n  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nCollecting chromadb==0.5.20 (from -r /kaggle/input/requirements-txt/requirements.txt (line 15))\n  Downloading chromadb-0.5.20-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 16)) (8.1.7)\nCollecting coloredlogs==15.0.1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 17))\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: dataclasses-json==0.6.7 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 18)) (0.6.7)\nRequirement already satisfied: Deprecated==1.2.15 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 19)) (1.2.15)\nRequirement already satisfied: distro==1.9.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 20)) (1.9.0)\nCollecting durationpy==0.9 (from -r /kaggle/input/requirements-txt/requirements.txt (line 21))\n  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\nCollecting fastapi==0.115.5 (from -r /kaggle/input/requirements-txt/requirements.txt (line 22))\n  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: filelock==3.16.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 23)) (3.16.1)\nRequirement already satisfied: flatbuffers==24.3.25 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 24)) (24.3.25)\nRequirement already satisfied: frozenlist==1.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 25)) (1.5.0)\nCollecting fsspec==2024.10.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 26))\n  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\nCollecting google-auth==2.36.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 27))\n  Downloading google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: googleapis-common-protos==1.66.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 28)) (1.66.0)\nCollecting grpcio==1.68.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 29))\n  Downloading grpcio-1.68.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: h11==0.14.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 30)) (0.14.0)\nRequirement already satisfied: httpcore==1.0.7 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 31)) (1.0.7)\nCollecting httptools==0.6.4 (from -r /kaggle/input/requirements-txt/requirements.txt (line 32))\n  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting httpx==0.28.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 33))\n  Downloading httpx-0.28.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 34)) (0.4.0)\nCollecting huggingface-hub==0.26.3 (from -r /kaggle/input/requirements-txt/requirements.txt (line 35))\n  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\nCollecting humanfriendly==10.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 36))\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: idna==3.10 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 37)) (3.10)\nRequirement already satisfied: importlib_metadata==8.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 38)) (8.5.0)\nRequirement already satisfied: importlib_resources==6.4.5 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 39)) (6.4.5)\nRequirement already satisfied: Jinja2==3.1.4 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 40)) (3.1.4)\nCollecting jiter==0.8.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 41))\n  Downloading jiter-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: joblib==1.4.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 42)) (1.4.2)\nRequirement already satisfied: jsonpatch==1.33 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 43)) (1.33)\nRequirement already satisfied: jsonpointer==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 44)) (3.0.0)\nCollecting kubernetes==31.0.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 45))\n  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting langchain==0.3.9 (from -r /kaggle/input/requirements-txt/requirements.txt (line 46))\n  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-community==0.3.8 (from -r /kaggle/input/requirements-txt/requirements.txt (line 47))\n  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\nCollecting langchain-core==0.3.21 (from -r /kaggle/input/requirements-txt/requirements.txt (line 48))\n  Downloading langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\nCollecting langchain-huggingface==0.1.2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 49))\n  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\nCollecting langchain-openai==0.2.10 (from -r /kaggle/input/requirements-txt/requirements.txt (line 50))\n  Downloading langchain_openai-0.2.10-py3-none-any.whl.metadata (2.6 kB)\nCollecting langchain-text-splitters==0.3.2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 51))\n  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\nCollecting langsmith==0.1.147 (from -r /kaggle/input/requirements-txt/requirements.txt (line 52))\n  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 53)) (3.0.0)\nRequirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 54)) (3.0.2)\nCollecting marshmallow==3.23.1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 55))\n  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 56)) (0.1.2)\nCollecting mmh3==5.0.1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 57))\n  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\nCollecting monotonic==1.6 (from -r /kaggle/input/requirements-txt/requirements.txt (line 58))\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 59)) (1.3.0)\nRequirement already satisfied: multidict==6.1.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 60)) (6.1.0)\nRequirement already satisfied: mypy-extensions==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 61)) (1.0.0)\nRequirement already satisfied: networkx==3.4.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 62)) (3.4.2)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 63)) (1.26.4)\nRequirement already satisfied: oauthlib==3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 64)) (3.2.2)\nCollecting onnxruntime==1.20.1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 65))\n  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nCollecting openai==1.55.3 (from -r /kaggle/input/requirements-txt/requirements.txt (line 66))\n  Downloading openai-1.55.3-py3-none-any.whl.metadata (24 kB)\nCollecting opentelemetry-api==1.28.2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 67))\n  Downloading opentelemetry_api-1.28.2-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 68))\n  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\nCollecting opentelemetry-exporter-otlp-proto-grpc==1.28.2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 69))\n  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-instrumentation==0.49b2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 70))\n  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-instrumentation-asgi==0.49b2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 71))\n  Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation-fastapi==0.49b2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 72))\n  Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-proto==1.28.2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 73))\n  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-sdk==1.28.2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 74))\n  Downloading opentelemetry_sdk-1.28.2-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-semantic-conventions==0.49b2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 75))\n  Downloading opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-util-http==0.49b2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 76))\n  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: orjson==3.10.12 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 77)) (3.10.12)\nRequirement already satisfied: overrides==7.7.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 78)) (7.7.0)\nRequirement already satisfied: packaging==24.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 79)) (24.2)\nRequirement already satisfied: pillow==11.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 80)) (11.0.0)\nCollecting posthog==3.7.4 (from -r /kaggle/input/requirements-txt/requirements.txt (line 81))\n  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\nCollecting propcache==0.2.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 82))\n  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nCollecting protobuf==5.29.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 83))\n  Downloading protobuf-5.29.0-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nRequirement already satisfied: pyasn1==0.6.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 84)) (0.6.1)\nRequirement already satisfied: pyasn1_modules==0.4.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 85)) (0.4.1)\nCollecting pydantic==2.10.2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 86))\n  Downloading pydantic-2.10.2-py3-none-any.whl.metadata (170 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.8/170.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pydantic-settings==2.6.1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 87))\n  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: pydantic_core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 88)) (2.27.1)\nRequirement already satisfied: Pygments==2.18.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 89)) (2.18.0)\nRequirement already satisfied: pypdf==5.1.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 90)) (5.1.0)\nCollecting PyPDF2==3.0.1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 91))\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nCollecting PyPika==0.48.9 (from -r /kaggle/input/requirements-txt/requirements.txt (line 92))\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting pyproject_hooks==1.2.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 93))\n  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\nCollecting python-dateutil==2.9.0.post0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 94))\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: python-dotenv==1.0.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 95)) (1.0.1)\nRequirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 96)) (6.0.2)\nRequirement already satisfied: regex==2024.11.6 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 97)) (2024.11.6)\nRequirement already satisfied: requests==2.32.3 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 98)) (2.32.3)\nCollecting requests-oauthlib==2.0.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 99))\n  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: requests-toolbelt==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 100)) (1.0.0)\nRequirement already satisfied: rich==13.9.4 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 101)) (13.9.4)\nRequirement already satisfied: rsa==4.9 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 102)) (4.9)\nRequirement already satisfied: safetensors==0.4.5 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 103)) (0.4.5)\nCollecting scikit-learn==1.5.2 (from -r /kaggle/input/requirements-txt/requirements.txt (line 104))\n  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting scipy==1.14.1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 105))\n  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sentence-transformers==3.3.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 106)) (3.3.1)\nCollecting setuptools==75.6.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 107))\n  Downloading setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\nRequirement already satisfied: shellingham==1.5.4 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 108)) (1.5.4)\nCollecting six==1.16.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 109))\n  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 110)) (1.3.1)\nCollecting SQLAlchemy==2.0.35 (from -r /kaggle/input/requirements-txt/requirements.txt (line 111))\n  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nCollecting starlette==0.41.3 (from -r /kaggle/input/requirements-txt/requirements.txt (line 112))\n  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 113)) (1.13.1)\nRequirement already satisfied: tenacity==9.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 114)) (9.0.0)\nRequirement already satisfied: threadpoolctl==3.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 115)) (3.5.0)\nRequirement already satisfied: tiktoken==0.8.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 116)) (0.8.0)\nCollecting tokenizers==0.20.3 (from -r /kaggle/input/requirements-txt/requirements.txt (line 117))\n  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 118)) (2.5.1+cu121)\nRequirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 119)) (4.67.1)\nCollecting transformers==4.46.3 (from -r /kaggle/input/requirements-txt/requirements.txt (line 120))\n  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting typer==0.14.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 121))\n  Downloading typer-0.14.0-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: typing-inspect==0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 122)) (0.9.0)\nRequirement already satisfied: typing_extensions==4.12.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 123)) (4.12.2)\nRequirement already satisfied: urllib3==2.2.3 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 124)) (2.2.3)\nCollecting uvicorn==0.32.1 (from -r /kaggle/input/requirements-txt/requirements.txt (line 125))\n  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\nCollecting uvloop==0.21.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 126))\n  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles==1.0.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 127))\n  Downloading watchfiles-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websocket-client==1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 128)) (1.8.0)\nRequirement already satisfied: websockets==14.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 129)) (14.1)\nRequirement already satisfied: wrapt==1.17.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 130)) (1.17.0)\nCollecting yarl==1.18.0 (from -r /kaggle/input/requirements-txt/requirements.txt (line 131))\n  Downloading yarl-1.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: zipp==3.21.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements-txt/requirements.txt (line 132)) (3.21.0)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.11.8->-r /kaggle/input/requirements-txt/requirements.txt (line 2)) (4.0.3)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio==4.6.2.post1->-r /kaggle/input/requirements-txt/requirements.txt (line 5)) (1.2.2)\nRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build==1.2.2.post1->-r /kaggle/input/requirements-txt/requirements.txt (line 10)) (2.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements-txt/requirements.txt (line 63)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements-txt/requirements.txt (line 63)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements-txt/requirements.txt (line 63)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements-txt/requirements.txt (line 63)) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements-txt/requirements.txt (line 63)) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements-txt/requirements.txt (line 63)) (2.4.1)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy==2.0.35->-r /kaggle/input/requirements-txt/requirements.txt (line 111)) (3.1.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->-r /kaggle/input/requirements-txt/requirements.txt (line 63)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->-r /kaggle/input/requirements-txt/requirements.txt (line 63)) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->-r /kaggle/input/requirements-txt/requirements.txt (line 63)) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy==1.26.4->-r /kaggle/input/requirements-txt/requirements.txt (line 63)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy==1.26.4->-r /kaggle/input/requirements-txt/requirements.txt (line 63)) (2024.2.0)\n\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'protobuf' candidate (version 5.29.0 at https://files.pythonhosted.org/packages/ee/2e/cc46181ddce0940647d21a8341bf2eddad247a5d030e8c30c7a342793978/protobuf-5.29.0-cp38-abi3-manylinux2014_x86_64.whl (from https://pypi.org/simple/protobuf/) (requires-python:>=3.8))\nReason for being yanked: https://github.com/protocolbuffers/protobuf/issues/19430#issuecomment-2518458119\u001b[0m\u001b[33m\n\u001b[0mDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\nDownloading aiohttp-3.11.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nDownloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading attrs-24.2.0-py3-none-any.whl (63 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\nDownloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chromadb-0.5.20-py3-none-any.whl (617 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m617.9/617.9 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading durationpy-0.9-py3-none-any.whl (3.5 kB)\nDownloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading grpcio-1.68.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx-0.28.0-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.6/447.6 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jiter-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.6/343.6 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain-0.3.9-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_community-0.3.8-py3-none-any.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.21-py3-none-any.whl (409 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\nDownloading langchain_openai-0.2.10-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading openai-1.55.3-py3-none-any.whl (389 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_api-1.28.2-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl (18 kB)\nDownloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl (18 kB)\nDownloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl (30 kB)\nDownloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl (16 kB)\nDownloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl (12 kB)\nDownloading opentelemetry_proto-1.28.2-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.28.2-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.8/118.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl (159 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.2/159.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_util_http-0.49b2-py3-none-any.whl (6.9 kB)\nDownloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading protobuf-5.29.0-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic-2.10.2-py3-none-any.whl (456 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.4/456.4 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\nDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\nDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\nDownloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading typer-0.14.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading yarl-1.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.4/319.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: PyPika\n  Building wheel for PyPika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for PyPika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53771 sha256=130dab3b9e79aa141750dd9225aeb0b438e870a53d4154755326f5781f3c2917\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built PyPika\nInstalling collected packages: PyPika, monotonic, durationpy, uvloop, uvicorn, SQLAlchemy, six, setuptools, pyproject_hooks, PyPDF2, protobuf, propcache, opentelemetry-util-http, mmh3, marshmallow, jiter, humanfriendly, httptools, grpcio, fsspec, certifi, bcrypt, backoff, attrs, asgiref, anyio, aiosignal, aiohappyeyeballs, yarl, watchfiles, starlette, python-dateutil, pydantic, opentelemetry-proto, opentelemetry-api, google-auth, coloredlogs, build, typer, requests-oauthlib, pydantic-settings, posthog, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, huggingface-hub, httpx, fastapi, aiohttp, tokenizers, opentelemetry-sdk, opentelemetry-instrumentation, openai, langsmith, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-core, opentelemetry-instrumentation-fastapi, langchain-text-splitters, langchain-openai, scipy, transformers, scikit-learn, onnxruntime, langchain, chroma-hnswlib, langchain-huggingface, langchain-community, chromadb\n  Attempting uninstall: SQLAlchemy\n    Found existing installation: SQLAlchemy 2.0.36\n    Uninstalling SQLAlchemy-2.0.36:\n      Successfully uninstalled SQLAlchemy-2.0.36\n  Attempting uninstall: six\n    Found existing installation: six 1.17.0\n    Uninstalling six-1.17.0:\n      Successfully uninstalled six-1.17.0\n  Attempting uninstall: setuptools\n    Found existing installation: setuptools 75.1.0\n    Uninstalling setuptools-75.1.0:\n      Successfully uninstalled setuptools-75.1.0\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.20.3\n    Uninstalling protobuf-3.20.3:\n      Successfully uninstalled protobuf-3.20.3\n  Attempting uninstall: propcache\n    Found existing installation: propcache 0.2.1\n    Uninstalling propcache-0.2.1:\n      Successfully uninstalled propcache-0.2.1\n  Attempting uninstall: marshmallow\n    Found existing installation: marshmallow 3.24.2\n    Uninstalling marshmallow-3.24.2:\n      Successfully uninstalled marshmallow-3.24.2\n  Attempting uninstall: jiter\n    Found existing installation: jiter 0.8.2\n    Uninstalling jiter-0.8.2:\n      Successfully uninstalled jiter-0.8.2\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.68.1\n    Uninstalling grpcio-1.68.1:\n      Successfully uninstalled grpcio-1.68.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.9.0\n    Uninstalling fsspec-2024.9.0:\n      Successfully uninstalled fsspec-2024.9.0\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2024.12.14\n    Uninstalling certifi-2024.12.14:\n      Successfully uninstalled certifi-2024.12.14\n  Attempting uninstall: attrs\n    Found existing installation: attrs 24.3.0\n    Uninstalling attrs-24.3.0:\n      Successfully uninstalled attrs-24.3.0\n  Attempting uninstall: anyio\n    Found existing installation: anyio 3.7.1\n    Uninstalling anyio-3.7.1:\n      Successfully uninstalled anyio-3.7.1\n  Attempting uninstall: aiosignal\n    Found existing installation: aiosignal 1.3.2\n    Uninstalling aiosignal-1.3.2:\n      Successfully uninstalled aiosignal-1.3.2\n  Attempting uninstall: aiohappyeyeballs\n    Found existing installation: aiohappyeyeballs 2.4.4\n    Uninstalling aiohappyeyeballs-2.4.4:\n      Successfully uninstalled aiohappyeyeballs-2.4.4\n  Attempting uninstall: yarl\n    Found existing installation: yarl 1.18.3\n    Uninstalling yarl-1.18.3:\n      Successfully uninstalled yarl-1.18.3\n  Attempting uninstall: python-dateutil\n    Found existing installation: python-dateutil 2.8.2\n    Uninstalling python-dateutil-2.8.2:\n      Successfully uninstalled python-dateutil-2.8.2\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.10.3\n    Uninstalling pydantic-2.10.3:\n      Successfully uninstalled pydantic-2.10.3\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.29.0\n    Uninstalling opentelemetry-api-1.29.0:\n      Successfully uninstalled opentelemetry-api-1.29.0\n  Attempting uninstall: google-auth\n    Found existing installation: google-auth 2.27.0\n    Uninstalling google-auth-2.27.0:\n      Successfully uninstalled google-auth-2.27.0\n  Attempting uninstall: typer\n    Found existing installation: typer 0.15.1\n    Uninstalling typer-0.15.1:\n      Successfully uninstalled typer-0.15.1\n  Attempting uninstall: requests-oauthlib\n    Found existing installation: requests-oauthlib 1.3.1\n    Uninstalling requests-oauthlib-1.3.1:\n      Successfully uninstalled requests-oauthlib-1.3.1\n  Attempting uninstall: pydantic-settings\n    Found existing installation: pydantic-settings 2.7.1\n    Uninstalling pydantic-settings-2.7.1:\n      Successfully uninstalled pydantic-settings-2.7.1\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.50b0\n    Uninstalling opentelemetry-semantic-conventions-0.50b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.50b0\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.27.0\n    Uninstalling huggingface-hub-0.27.0:\n      Successfully uninstalled huggingface-hub-0.27.0\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.28.1\n    Uninstalling httpx-0.28.1:\n      Successfully uninstalled httpx-0.28.1\n  Attempting uninstall: aiohttp\n    Found existing installation: aiohttp 3.11.10\n    Uninstalling aiohttp-3.11.10:\n      Successfully uninstalled aiohttp-3.11.10\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.29.0\n    Uninstalling opentelemetry-sdk-1.29.0:\n      Successfully uninstalled opentelemetry-sdk-1.29.0\n  Attempting uninstall: openai\n    Found existing installation: openai 1.57.4\n    Uninstalling openai-1.57.4:\n      Successfully uninstalled openai-1.57.4\n  Attempting uninstall: langsmith\n    Found existing installation: langsmith 0.2.3\n    Uninstalling langsmith-0.2.3:\n      Successfully uninstalled langsmith-0.2.3\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.31\n    Uninstalling langchain-core-0.3.31:\n      Successfully uninstalled langchain-core-0.3.31\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.3\n    Uninstalling langchain-text-splitters-0.3.3:\n      Successfully uninstalled langchain-text-splitters-0.3.3\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.13.1\n    Uninstalling scipy-1.13.1:\n      Successfully uninstalled scipy-1.13.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.15\n    Uninstalling langchain-0.3.15:\n      Successfully uninstalled langchain-0.3.15\n  Attempting uninstall: langchain-community\n    Found existing installation: langchain-community 0.3.15\n    Uninstalling langchain-community-0.3.15:\n      Successfully uninstalled langchain-community-0.3.15\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.2.0 requires fsspec[http]<=2024.9.0,>=2023.1.0, but you have fsspec 2024.10.0 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.0 which is incompatible.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.0 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.27.0, but you have google-auth 2.36.0 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\ns3fs 2024.9.0 requires fsspec==2024.9.0.*, but you have fsspec 2024.10.0 which is incompatible.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.0 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\ntensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.0 which is incompatible.\ntweepy 4.14.0 requires requests-oauthlib<2,>=1.2.0, but you have requests-oauthlib 2.0.0 which is incompatible.\nydata-profiling 4.12.1 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed PyPDF2-3.0.1 PyPika-0.48.9 SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.3 aiohttp-3.11.8 aiosignal-1.3.1 anyio-4.6.2.post1 asgiref-3.8.1 attrs-24.2.0 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 certifi-2024.8.30 chroma-hnswlib-0.7.6 chromadb-0.5.20 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.5 fsspec-2024.10.0 google-auth-2.36.0 grpcio-1.68.0 httptools-0.6.4 httpx-0.28.0 huggingface-hub-0.26.3 humanfriendly-10.0 jiter-0.8.0 kubernetes-31.0.0 langchain-0.3.9 langchain-community-0.3.8 langchain-core-0.3.21 langchain-huggingface-0.1.2 langchain-openai-0.2.10 langchain-text-splitters-0.3.2 langsmith-0.1.147 marshmallow-3.23.1 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.20.1 openai-1.55.3 opentelemetry-api-1.28.2 opentelemetry-exporter-otlp-proto-common-1.28.2 opentelemetry-exporter-otlp-proto-grpc-1.28.2 opentelemetry-instrumentation-0.49b2 opentelemetry-instrumentation-asgi-0.49b2 opentelemetry-instrumentation-fastapi-0.49b2 opentelemetry-proto-1.28.2 opentelemetry-sdk-1.28.2 opentelemetry-semantic-conventions-0.49b2 opentelemetry-util-http-0.49b2 posthog-3.7.4 propcache-0.2.0 protobuf-5.29.0 pydantic-2.10.2 pydantic-settings-2.6.1 pyproject_hooks-1.2.0 python-dateutil-2.9.0.post0 requests-oauthlib-2.0.0 scikit-learn-1.5.2 scipy-1.14.1 setuptools-75.6.0 six-1.16.0 starlette-0.41.3 tokenizers-0.20.3 transformers-4.46.3 typer-0.14.0 uvicorn-0.32.1 uvloop-0.21.0 watchfiles-1.0.0 yarl-1.18.0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!pip install --upgrade transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:53:52.989978Z","iopub.status.idle":"2025-01-21T12:53:52.990227Z","shell.execute_reply":"2025-01-21T12:53:52.990128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade langchain\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T12:53:52.990760Z","iopub.status.idle":"2025-01-21T12:53:52.991055Z","shell.execute_reply":"2025-01-21T12:53:52.990954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T13:24:59.722656Z","iopub.execute_input":"2025-01-21T13:24:59.723013Z","iopub.status.idle":"2025-01-21T13:25:04.483235Z","shell.execute_reply.started":"2025-01-21T13:24:59.722986Z","shell.execute_reply":"2025-01-21T13:25:04.482328Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\neﬀects. *e review was conducted in three stages: planning,\nperforming, and reporting the systematic review.\n2.1. Planning the Review. As previous reviews about AIwere\nconducted in the physical sciences [16, 17], the study aimed\nto conduct a review in the ﬁeld of the social sciences.\n*e Web of Science database and the Social Science\nCitation Index (SSCI) journals were selected for the search\nfor desired articles published from 2010 to 2020. Articles\n\neﬀects. *e review was conducted in three stages: planning,\nperforming, and reporting the systematic review.\n2.1. Planning the Review. As previous reviews about AIwere\nconducted in the physical sciences [16, 17], the study aimed\nto conduct a review in the ﬁeld of the social sciences.\n*e Web of Science database and the Social Science\nCitation Index (SSCI) journals were selected for the search\nfor desired articles published from 2010 to 2020. Articles\n\neﬀects. *e review was conducted in three stages: planning,\nperforming, and reporting the systematic review.\n2.1. Planning the Review. As previous reviews about AIwere\nconducted in the physical sciences [16, 17], the study aimed\nto conduct a review in the ﬁeld of the social sciences.\n*e Web of Science database and the Social Science\nCitation Index (SSCI) journals were selected for the search\nfor desired articles published from 2010 to 2020. Articles\n\nQuestion: What is the main conclusion of the research?\nHelpful Answer:\n\neﬀects. *e review was conducted in three stages: planning,\nperforming, and reporting the systematic review.\n2.2. Planning the Review. As previous reviews about AIwere\nconducted in the physical sciences [16, 17], the study aimed\nto conduct a review in the ﬁeld of the social sciences.\n*e Web of Science database and the Social Science\nCitation Index (SSCI) journals were selected for the search\nfor desired articles published from 2010 to 2020. Articles\n\neﬀect\n\nSource Documents:\n- Page 1: eﬀects. *e review was conducted in three stages: planning,\nperforming, and reporting the systematic review.\n2.1. Planning the Review. As previous reviews about AIwere\nconducted in the physical science...\n- Page 1: eﬀects. *e review was conducted in three stages: planning,\nperforming, and reporting the systematic review.\n2.1. Planning the Review. As previous reviews about AIwere\nconducted in the physical science...\n- Page 1: eﬀects. *e review was conducted in three stages: planning,\nperforming, and reporting the systematic review.\n2.1. Planning the Review. As previous reviews about AIwere\nconducted in the physical science...\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}